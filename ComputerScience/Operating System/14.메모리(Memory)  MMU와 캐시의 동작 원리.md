# 메모리(Memory): MMU와 캐시의 동작 원리
## 메인 메모리 (Main Memory)

메인 메모리는 **CPU가 직접 접근할 수 있는 주기억장치**이다.

프로세스가 실행되기 위해서는 프로그램이 반드시 메모리에 올라와 있어야 하며, 메모리는 **주소가 할당된 연속적인 바이트들의 집합**으로 구성된다.

CPU는 레지스터가 가리키는 주소를 기반으로 메모리에 접근하여 다음에 실행할 명령어를 가져오고, 명령어 수행에 필요한 데이터 또한 메모리에서 읽어온다.

하지만 페이징과 세마포어 파트에서 학습한 가상 메모리를 통해서 메모리를 관리하기 때문에 CPU가 사용하는 주소와 실제 메모리 주소가 다르다.

이때 주소 변환을 담당하는 하드웨어가 바로 **MMU**이다.

## MMU (Memory Management Unit)

MMU는 **논리 주소(Logical Address)를 물리 주소(Physical Address)로 변환해주는 하드웨어 장치**이다.

프로그램은 가상 주소 공간을 기준으로 동작하고, 실제 데이터는 물리 메모리에 저장되어 있다. 

이 둘을 연결해주는 것이 바로 MMU이다.

### 왜 가상 주소가 필요한가?

물리 메모리는 한정되어 있지만, 프로세스는 자신만의 독립적인 메모리 공간을 가진 것처럼 동작해야 한다.

이를 위해 운영체제는 **가상 주소 공간(Virtual Address Space)** 을 제공한다.

가상 메모리 주소의 장점은 아래와 같다.

- 사용자가 물리 주소를 직접 관리할 필요가 없다.
- 프로세스마다 독립적인 주소 공간을 가질 수 있다.
- 실제 메모리 크기보다 큰 프로그램도 실행할 수 있다. (가상 메모리 활용)

이 과정에서 빠른 주소 변환이 필수적이며, 이를 담당하는 것이 바로 MMU이다.

MMU가 없다면 프로그램이 직접 물리 주소를 다뤄야 하며, 메모리 보호가 어려워진다. 그렇기 때문에 프로세스 간 충돌 위험이 커지고, 메모리 관리 부담이 커지게 된다.

### MMU의 메모리 보호 기능

프로세스는 서로의 메모리 영역을 침범해서는 안 된다. 이를 위해 MMU는 **메모리 보호 기능**을 제공한다.

`base`와 `limit` 레지스터를 이용하는 방법을 사용한다.

- **base 레지스터** : 프로세스가 시작하는 물리 주소
- **limit 레지스터** : 프로세스의 메모리 크기

프로세스가 접근 가능한 주소 범위는 다음과 같다.

```
base ≤ x <base + limit
```

이 범위를 벗어난 접근이 발생하면 MMU는 **trap(예외)** 을 발생시켜 잘못된 접근을 차단한다.

또한 보안을 위해 base와 limit 레지스터는 **커널 모드에서만 수정 가능**하도록 설계되어 있다.

## 메모리 과할당 (Over-Allocating)

메모리 과할당이란 **실제 물리 메모리 크기보다 더 큰 가상 메모리를 프로세스에 제공하는 것**을 의미한다.

운영체제는 가상 메모리 기법을 이용해 각 프로세스가 마치 충분히 큰 메모리를 가진 것처럼 보이게 만든다.

이 때 모든 페이지가 실제 메모리에 올라와 있는 것은 아니며, 필요한 순간에만 디스크에서 메모리로 적재된다. 이를 **요구페이징(Demand Paging)** 이라고 한다.

모든 프로그램은 자신의 전체 메모리 공간을 항상 사용하는 것은 아니다.

예를 들어:

- 함수 A를 실행하는 동안 함수 B의 코드 영역은 사용되지 않는다.
- 큰 배열을 선언했더라도, 실제로 일부만 접근할 수도 있다.

이런 특성을 이용해 운영체제는:

- 당장 필요한 페이지들만 메모리에 올리고
- 나머지는 디스크에 둔다

그래서 실제 메모리보다 더 많은 메모리를 “할당한 것처럼” 보이게 할 수 있다.

메모리 과할당시 페이지 교체(Page Replacement)는 필수 상황이다.

프로세스 실행 중 페이지 폴트가 발생해, 해당 페이지를 디스크에서 가져와야하는 경우에 메모리에 빈 프레임이 없다면 페이지 교체를 해야한다. 빈 프레임이 없을 때, 현재 메모리에 존재하는 페이지 중 하나를 선택해 제거하고 그 자리에 새로운 페이지를 적재해야한다.

이는 성능에 치명적인 역할을 한다.

그 이유는 바로 디스크 I/O 때문이다.

- 메모리 접근 속도: 나노초(ns)
- 디스크 접근 속도: 밀리초(ms)

메모리와 디스크 접근 속도는 수십만 배 이상 난다.

따라서 페이지 교체가 자주 발생하면 시스템 성능이 급격히 저하되고, 이 현상이 심해지면 Thrashing이 발생하게 되는 것이다.(페이지 교체에 대부분의 시간을 쓰는 상태)

### 오버헤드 감소 방법

이처럼 페이지 교체가 많이 이루어지면, 디스크 입출력 연산이 많이 발생하게 되어 오버헤드 문제가 발생한다.

이를 해결하는 방법으로는 두가지가 있다.

### 1. Dirty Bit (변경 비트)

모든 페이지에 변경 여부를 나타내는 비트를 둔다.

- **1 (set)** → 메모리에서 수정된 페이지 → 디스크에 기록 필요
- **0 (clear)** → 수정되지 않음 → 디스크 기록 불필요

이 기법을 사용하면 **불필요한 디스크 write를 줄일 수 있고, 디스크 접근 횟수를 절반 가까이 줄일 수 있다.**

즉, **디스크 I/O 오버헤드를 감소시키는 핵심 장치이다.**

### 2. 페이지 교체 알고리즘의 선택

궁극적인 해결책은 **페이지 폴트 자체를 줄이는 것**이다.

페이지 교체 파트에서 학습한 페이지 교체 알고리즘들을 잘 선택해 사용하는 것이다.

## 캐시 메모리 (Cache Memory)

캐시 메모리는 **주기억장치(메인 메모리)의 일부 데이터를 임시로 저장해두는 고속 메모리**이다.

CPU와 메인 메모리 사이에는 큰 속도 차이가 존재한다.

- CPU 속도 → 매우 빠름 (ns 단위)
- DRAM(메인 메모리) → 상대적으로 느림
- 디스크 → 훨씬 느림

이 속도 차이 때문에 CPU가 메모리에 직접 접근할 때마다 병목이 발생한다. 이를 해결하기 위해 CPU와 메인 메모리 사이에 **더 빠른 소형 메모리(캐시)** 를 둔다.

캐시는 **SRAM(Static RAM)**으로 구성되어 있다. SRAM은 플립플롭(Flip-Flop) 회로로 구성되며, DRAM보다 구조가 복잡하지만 훨씬 빠르다. 가격이 비싸고 용량이 작지만 매우 빠른 메모리이다.

## CPU와 캐시의 동작 과정

CPU가 어떤 주소의 데이터를 요청하면 다음과 같은 과정이 일어난다.

1. **CPU가 주소 전달**
2. **캐시에 해당 데이터가 있는지 확인**
- **Hit (적중)**
    
    → 캐시에 데이터 존재
    
    → 바로 CPU에 전달
    
    → 빠르게 처리 완료
    
- **Miss (미스)**
    
    → 캐시에 없음
    
    → 메인 메모리에서 데이터 가져옴
    
    → 해당 데이터를 캐시에 저장
    
    → CPU에 전달
    

### 캐시 적중률(Hit Ratio)

이처럼 캐시를 잘 활용하여 적중(Hit)했을 경우 비용을 많이 줄일 수 있다.

캐시의 성능은 정중률(Hit Ratio)로 판단된다.

캐시 적중률이 높을 수록

- 메모리 접근 횟수가 줄어든다.
- CPU 대기 시간이 줄어든다.
- 전체 시스템 성능이 향상된다.

이처럼 캐시의 핵심은 적중률을 얼마나 높이느냐이다.

## 지역성(Locality)의 원리

적중률을 높일 수 있는 이유는 프로그램이 특정한 패턴을 가지고 메모리에 접근하기 때문이다.

즉, 프로그램은 메모리 전체를 고르게 사용하는 것이 아니라 특정 영역을 집중적으로 참조하는 경향이 있는데 이를 **지역성(Locality)** 이라고 한다.

### 시간 지역성(Temporal Locality)

최근에 사용된 데이터는 가까운 미래에 다시 사용될 가능성이 높다는 특성을 시간 지역성(Temporal Locality)이라고 한다.

- 반복문 내부 변수
- 자주 호출되는 함수
- 스택에 있는 지역 변수

### 공간 지역성(Spatial Locality)

특정 주소를 참조하면 그 주변 주소도 함께 참조될 가능성이 높다는 특성을 공간 지역성(Spatial Locality)이라고 한다.

- 배열 순차 접근
- 구조체 멤버 접근
- 명령어의 순차 실행

## 캐싱 라인 (**Caching line**)

캐싱 라인은 **캐시가 메인 메모리로부터 한 번에 가져와 저장하는 기본 단위**이다. 

CPU가 특정 메모리 주소의 데이터를 요청하면, 해당 데이터 한 바이트만 가져오는 것이 아니라 그 주소를 포함한 일정 크기의 블록을 함께 가져온다. 이는 CPU가 캐시 메모리에서 필요한 데이터를 빠르게 접근하기 위해 만들어졌다.

캐싱 라인으로 묶이는 묶음의 방식이 여러가지가 있는데, 대표적으로는 Full Associative, Set Associative, Direct Map 등이 있다.

예를 들어, 캐싱 라인의 크기가 64바이트라면

- CPU가 1바이트를 요청하더라도
- 해당 주소를 포함한 64바이트 전체가 캐시에 적재된다.

이는 **공간 지역성(Spatial Locality)**을 활용하기 위함이다.

### 캐싱 라인이 존재하는 이유

캐시 메모리는 메인 메모리에 비해 크기가 매우 작기 때문에, 메인 메모리와 1:1 대응이 불가능하다. 또한 CPU는 데이터를 캐시에서 매우 빠르게 바로 찾을 수 없다면 비효율 적이게 된다.

따라서 데이터를 블록 단위(캐싱 라인)로 묶어 전송하고 주소 일부(Tag)를 함께 저장하며 하드웨어 매핑 방식을 통해 빠르게 탐색한다.

이 구조 덕분에 캐시는 높은 적중률과 빠른 접근 속도를 유지할 수 있다.

---

## iOS 개발자가 알면 좋을 것들

iOS 개발을 하다보면 많은 UIImage들을 캐시를 통해 저장해야하는 경우가 있다. 

이처럼 캐시가 필요할 때 주로 NSCache를 사용하는데, 이는 NSCache는 메모리 부족시 자동 제거, 시스템 메모리 상황을 인지해 주기 때문이다. 만약 NSCache가 아니라 직접 캐싱을 할 경우 메모리 압박이 발생해 페이지 교체 증가, Thrashing 위험, iOS에서 메모리 경고 발생등이 일어날 수 있게 된다.
